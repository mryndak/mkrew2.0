# Struktura projektu mkrew2

## ğŸ“‹ Cel projektu

**GÅ‚Ã³wny cel:** Opracowanie systemu prognostycznego przewidujÄ…cego zapotrzebowanie na krew w Regionalnych Centrach Krwiodawstwa i Krwiolecznictwa (RCKiK) w Polsce, umoÅ¼liwiajÄ…cego wczesne wykrywanie potencjalnych niedoborÃ³w.

### Kluczowe funkcjonalnoÅ›ci:
- âœ… Automatyczne zbieranie danych o stanach magazynowych krwi z 21 RCKiK w Polsce
- âœ… Przechowywanie historycznych danych o 8 grupach krwi (A+, A-, B+, B-, AB+, AB-, O+, O-)
- âœ… Prognozowanie niedoborÃ³w krwi przy uÅ¼yciu modeli ML (ARIMA, Prophet)
- âœ… System wczesnego ostrzegania o potencjalnych niedoborach
- âœ… Dashboard z wizualizacjami i prognozami

## ğŸ—ï¸ Architektura projektu

```
mkrew2/
â”œâ”€â”€ /db                 # âš ï¸ Liquibase migrations - GÅÃ“WNY katalog bazy danych!
â”œâ”€â”€ /backend            # REST API (Spring Boot)
â”œâ”€â”€ /scraper            # Serwis scrapujÄ…cy dane RCKiK
â”œâ”€â”€ /frontend           # Aplikacja webowa (Astro)
â””â”€â”€ /ml                 # Modele uczenia maszynowego (planowany)
```

## ğŸ“ SzczegÃ³Å‚owa struktura katalogÃ³w

### ğŸ“‚ `/db` - Baza danych (Liquibase)
**âš ï¸ WAÅ»NE: To jest JEDYNE miejsce na migracje bazy danych!**

```
db/
â”œâ”€â”€ changelog/
â”‚   â”œâ”€â”€ db.changelog-master.yaml          # Master changelog
â”‚   â””â”€â”€ changes/
â”‚       â”œâ”€â”€ 001-create-rckik-table.yaml
â”‚       â”œâ”€â”€ 002-create-blood-inventory-record-table.yaml
â”‚       â”œâ”€â”€ 003-create-scraping-log-table.yaml
â”‚       â”œâ”€â”€ 004-create-users-table.yaml
â”‚       â””â”€â”€ 005-create-forecast-tables.yaml
â””â”€â”€ erd-diagram.drawio
```

**Technologia:** Liquibase (NIE Flyway!)
**Format:** YAML changesets
**Baza danych:** PostgreSQL

**Tabele:**
- `rckik` - Regionalne Centra Krwiodawstwa (21 centrÃ³w)
- `blood_inventory_record` - Historyczne stany magazynowe krwi
- `scraping_log` - Logi scrapingu danych
- `users` - UÅ¼ytkownicy systemu (ADMIN, USER_DATA)
- `forecast_model` - Modele ML (ARIMA, Prophet, SARIMA, LSTM)
- `forecast_request` - Å»Ä…dania prognozy
- `forecast_result` - Wyniki prognoz z interwaÅ‚ami ufnoÅ›ci

### ğŸ“‚ `/backend` - Backend API
```
backend/
â”œâ”€â”€ src/main/java/pl/mkrew/backend/
â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â”œâ”€â”€ entity/          # JPA entities
â”‚   â”‚   â””â”€â”€ enums/           # BloodType, InventoryStatus, UserRole, ForecastModelType
â”‚   â”œâ”€â”€ repository/          # Spring Data JPA repositories
â”‚   â”œâ”€â”€ service/             # Business logic + ML service client
â”‚   â”œâ”€â”€ controller/          # REST endpoints
â”‚   â”œâ”€â”€ security/            # JWT + Spring Security
â”‚   â”œâ”€â”€ dto/                 # Data Transfer Objects
â”‚   â””â”€â”€ config/              # Configuration classes
â””â”€â”€ build.gradle.kts
```

**Technologia:**
- Spring Boot 3.4.1
- Java 21
- Gradle
- PostgreSQL
- JWT Authentication
- Spring Security

**Port:** 8081

**Kluczowe endpointy:**
- `POST /api/auth/login` - Logowanie (public)
- `GET /api/blood-inventory/**` - Dane o krwi (USER_DATA, ADMIN)
- `POST /api/admin/scraper/trigger` - Trigger scrapingu (ADMIN)
- `POST /api/forecast/create` - Uruchom prognozÄ™ (ADMIN)
- `GET /api/forecast/{id}` - Pobierz prognozÄ™ (ADMIN)

**UÅ¼ytkownicy domyÅ›lni:**
- Admin: `admin` / `admin123` (rola: ADMIN)
- User: `user` / `user123` (rola: USER_DATA)

### ğŸ“‚ `/scraper` - Web Scraper
```
scraper/
â”œâ”€â”€ src/main/java/pl/mkrew/scraper/
â”‚   â”œâ”€â”€ domain/entity/       # JPA entities (RCKiK, BloodInventoryRecord, ScrapingLog)
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â”œâ”€â”€ strategy/        # Strategie scrapingu dla rÃ³Å¼nych RCKiK
â”‚   â”‚   â”‚   â”œâ”€â”€ RzeszowScraperStrategy.java
â”‚   â”‚   â”‚   â”œâ”€â”€ KrakowScraperStrategy.java
â”‚   â”‚   â”‚   â””â”€â”€ WroclawScraperStrategy.java
â”‚   â”‚   â””â”€â”€ dto/
â”‚   â”œâ”€â”€ service/             # ScraperService
â”‚   â”œâ”€â”€ scheduler/           # Quartz scheduled jobs
â”‚   â””â”€â”€ controller/          # REST endpoints do manualnego triggera
â””â”€â”€ build.gradle.kts
```

**Technologia:**
- Spring Boot
- Java 21
- Jsoup (web scraping)
- Quartz Scheduler
- PostgreSQL

**Port:** 8080

**FunkcjonalnoÅ›Ä‡:**
- Automatyczne zbieranie danych ze stron RCKiK (cron: 8:00, 14:00, 20:00)
- Parsowanie HTML dla kaÅ¼dego centrum (osobne strategie)
- Zapis stanÃ³w magazynowych do bazy danych
- Logowanie statusu scrapingu

**Endpointy:**
- `POST /api/scraper/trigger` - Manual trigger scrapingu
- `GET /api/scraper/status` - Status ostatniego scrapingu

### ğŸ“‚ `/frontend` - Frontend (Astro)
```
frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/          # React/Astro components
â”‚   â”œâ”€â”€ layouts/             # Layout templates
â”‚   â”œâ”€â”€ pages/               # Astro pages (routing)
â”‚   â””â”€â”€ styles/              # CSS/Tailwind
â”œâ”€â”€ public/                  # Static assets
â””â”€â”€ package.json
```

**Technologia:**
- Astro framework
- React (dla komponentÃ³w interaktywnych)
- Tailwind CSS
- TypeScript

**Port:** (do ustalenia)

**Planowane funkcje:**
- Dashboard z mapÄ… Polski i stanami RCKiK
- Wizualizacje szeregÃ³w czasowych
- WyÅ›wietlanie prognoz ML
- System alertÃ³w o niedoborach
- Panel administratora

### ğŸ“‚ `/ml` - Machine Learning (planowany)
```
ml/
â”œâ”€â”€ models/                  # Wytrenowane modele
â”œâ”€â”€ notebooks/               # Jupyter notebooks (eksperymentowanie)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing/       # Przygotowanie danych
â”‚   â”œâ”€â”€ training/            # Trenowanie modeli
â”‚   â””â”€â”€ api/                 # REST API dla prognoz
â”œâ”€â”€ requirements.txt
â””â”€â”€ Dockerfile
```

**Technologia (planowana):**
- Python 3.10+
- Flask/FastAPI (REST API)
- statsmodels (ARIMA)
- Prophet (Facebook)
- scikit-learn
- pandas, numpy

**Port:** 5000

**Modele do implementacji:**
1. **ARIMA** - AutoRegressive Integrated Moving Average (priorytet 1)
2. **Prophet** - Facebook's forecasting tool (priorytet 2)
3. **SARIMA** - Seasonal ARIMA (przyszÅ‚oÅ›Ä‡)
4. **LSTM** - Deep learning (przyszÅ‚oÅ›Ä‡)

**API endpoints (planowane):**
- `POST /api/forecast` - UtwÃ³rz prognozÄ™
  - Input: dane historyczne + parametry modelu
  - Output: prognozy z interwaÅ‚ami ufnoÅ›ci

## ğŸ”„ PrzepÅ‚yw danych

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTP      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RCKiK   â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ SCRAPER  â”‚
â”‚ websites â”‚               â”‚  :8080   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                 â”‚ INSERT
                                 â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚PostgreSQLâ”‚â—„â”€â”€â”€â”€â”€â”
                           â”‚   DB     â”‚      â”‚
                           â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â”‚
                                 â”‚ SELECT    â”‚ INSERT
                                 â–¼           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     REST      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ FRONTEND â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚ BACKEND  â”‚â”€â”€â”€â”€â”€â”˜
â”‚  Astro   â”‚               â”‚  :8081   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                 â”‚ REST
                                 â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚    ML    â”‚
                           â”‚  :5000   â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SzczegÃ³Å‚owy flow:
1. **Scraper** â†’ Pobiera dane ze stron RCKiK (codziennie 8:00, 14:00, 20:00)
2. **Scraper** â†’ Zapisuje do `blood_inventory_record` w PostgreSQL
3. **Backend** â†’ UdostÄ™pnia dane przez REST API
4. **Frontend** â†’ WyÅ›wietla dane uÅ¼ytkownikowi
5. **Admin** â†’ WywoÅ‚uje prognozÄ™ przez `/api/forecast/create`
6. **Backend** â†’ Pobiera dane historyczne z bazy
7. **Backend** â†’ WysyÅ‚a request do ML service
8. **ML Service** â†’ Generuje prognozÄ™ (ARIMA/Prophet)
9. **Backend** â†’ Zapisuje wyniki do `forecast_result`
10. **Frontend** â†’ WyÅ›wietla prognozy

## ğŸ” BezpieczeÅ„stwo

### Autentykacja i autoryzacja:
- **JWT tokens** (Bearer authentication)
- **Spring Security** + BCrypt password encoding
- **Role-based access control (RBAC)**

### Role uÅ¼ytkownikÃ³w:
1. **ADMIN**
   - PeÅ‚ny dostÄ™p do wszystkich endpointÃ³w
   - MoÅ¼e uruchamiaÄ‡ scraping i prognozy
   - ZarzÄ…dzanie uÅ¼ytkownikami

2. **USER_DATA**
   - DostÄ™p tylko do odczytu danych o krwi
   - Brak dostÄ™pu do prognoz i administracji

## ğŸ—„ï¸ Konfiguracja

### Backend (`application.properties`):
```properties
server.port=8081
spring.datasource.url=jdbc:postgresql://localhost:5432/mkrew
jwt.secret=your-256-bit-secret-key-change-this-in-production
scraper.service.url=http://localhost:8080
ml.service.url=http://localhost:5000
```

### Scraper (`application.properties`):
```properties
server.port=8080
spring.datasource.url=jdbc:postgresql://localhost:5432/mkrew
```

### Docker Compose:
```yaml
services:
  - postgres:5432 (baza danych)
  - backend:8081 (API)
  - scraper:8080 (scraping service)
  - frontend (do skonfigurowania)
```

## ğŸš€ Uruchomienie projektu

### Wymagania:
- Java 21
- PostgreSQL 15+
- Gradle 8+
- Node.js 18+ (dla frontendu)
- Python 3.10+ (dla ML, w przyszÅ‚oÅ›ci)

### Kroki:
1. Uruchom PostgreSQL
2. Uruchom migracje Liquibase (automatycznie przy starcie backendu/scrapera)
3. Uruchom scraper: `cd scraper && ./gradlew bootRun`
4. Uruchom backend: `cd backend && ./gradlew bootRun`
5. Uruchom frontend: `cd frontend && npm run dev`
6. (PrzyszÅ‚oÅ›Ä‡) Uruchom ML service: `cd ml && python app.py`

## ğŸ“Š Modele ML - SzczegÃ³Å‚y

### ARIMA (priorytet 1):
- **Zastosowanie:** Prognozowanie stanÃ³w magazynowych krwi
- **Parametry:** p, d, q (zapisane w `forecast_model.model_parameters`)
- **Horyzont:** 1-7 dni (krÃ³tkoterminowy)
- **Input:** Szereg czasowy stanÃ³w dla danej grupy krwi w danym RCKiK
- **Output:** Prognoza + przedziaÅ‚y ufnoÅ›ci

### Prophet (priorytet 2):
- **Zastosowanie:** DÅ‚ugoterminowe prognozy z sezonowoÅ›ciÄ…
- **UwzglÄ™dnia:** ÅšwiÄ™ta, weekendy, wzorce roczne
- **Horyzont:** 1-4 tygodnie (Å›rednioterminowy)

### System alertÃ³w (planowany):
- **Poziom 1 (Info):** Spadek do stanu Å›redniego w ciÄ…gu 7 dni
- **Poziom 2 (Warning):** Spadek do stanu niskiego w ciÄ…gu 7 dni
- **Poziom 3 (Critical):** Spadek do stanu niskiego w ciÄ…gu 3 dni

## ğŸ“ Dodatkowe informacje

### RCKiK (21 centrÃ³w):
BiaÅ‚ystok, Bydgoszcz, GdaÅ„sk, Kalisz, Katowice, Kielce, KrakÃ³w, Lublin, ÅÃ³dÅº, Olsztyn, Opole, PoznaÅ„, RacibÃ³rz, Radom, RzeszÃ³w, SÅ‚upsk, Szczecin, WaÅ‚brzych, Warszawa, WrocÅ‚aw, Zielona GÃ³ra

### Grupy krwi (8 typÃ³w):
A+, A-, B+, B-, AB+, AB-, O+, O-

### Kodowanie stanÃ³w magazynowych:
- `CRITICALLY_LOW` = Stan krytycznie niski
- `LOW` = Stan niski
- `MEDIUM` = Stan Å›redni
- `SATISFACTORY` = Stan zadowalajÄ…cy
- `OPTIMAL` = Stan optymalny

## ğŸ”§ Technologie - Podsumowanie

| Komponent | Technologia | Port | Cel |
|-----------|-------------|------|-----|
| Database | PostgreSQL + Liquibase | 5432 | Przechowywanie danych |
| Backend | Spring Boot 3.4 + Java 21 | 8081 | REST API + logika biznesowa |
| Scraper | Spring Boot + Jsoup + Quartz | 8080 | Web scraping RCKiK |
| Frontend | Astro + React + Tailwind | TBD | UI/Dashboard |
| ML Service | Python + Flask/FastAPI + Prophet/ARIMA | 5000 | Prognozy ML |

## ğŸ“š Dokumentacja dodatkowa

- `metodologia.md` - PeÅ‚na metodologia badawcza projektu
- `db/erd-diagram.drawio` - Diagram ERD bazy danych
- `backend/ARCHITECTURE.md` - Architektura backendu (jeÅ›li istnieje)
- `backend/IMPLEMENTATION_GUIDE.md` - Przewodnik implementacji

## âš ï¸ WaÅ¼ne uwagi dla AI/AsystentÃ³w

1. **Migracje bazy danych:** UÅ¼ywamy Liquibase (NIE Flyway!), pliki YAML w `/db/changelog/changes/`
2. **Struktura katalogÃ³w:** `/db` jest GÅÃ“WNYM miejscem dla bazy, NIE `backend/src/main/resources/db`
3. **Projekt jest w trakcie rozwoju:** ModuÅ‚ `/ml` nie istnieje jeszcze
4. **Testowe dane:** Admin user juÅ¼ utworzony w migracji 004
5. **API prognozy:** Backend gotowy, czeka na implementacjÄ™ ML service w Pythonie

---

**Ostatnia aktualizacja:** 2025-10-03
**Status projektu:** Backend + Scraper dziaÅ‚ajÄ…, Frontend + ML w trakcie rozwoju
